# ğŸ“˜ Script 2: `data.py`

## Overview

This script handles **everything related to data**: loading, preprocessing, tokenization, caching, and creating data loaders. It's the bridge between your raw CSV file and the tensors that PyTorch needs.

**Why this script is critical:**
- Tokenization is SLOW (90+ seconds per run)
- Caching saves you 10-72 minutes per experiment
- Proper data handling prevents subtle bugs

---

## Section 1: Imports and Environment Setup

```python
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # MUST be before transformers import!

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from transformers import AutoTokenizer
from sklearn.model_selection import KFold
import pickle
import hashlib
from typing import List, Tuple, Optional, Dict
```

### What This Does:

1. **`TOKENIZERS_PARALLELISM = "false"`**: Prevents a warning about forking processes
2. **Import order matters**: Environment variables must be set BEFORE importing transformers

### Why This Specific Order:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THE TOKENIZER PARALLELISM PROBLEM                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ What happens without the fix:                                              â”‚
â”‚                                                                             â”‚
â”‚ 1. HuggingFace tokenizers use multiple threads by default                  â”‚
â”‚ 2. PyTorch DataLoader ALSO uses multiple workers                           â”‚
â”‚ 3. When DataLoader forks processes, each gets a copy of tokenizer          â”‚
â”‚ 4. Multiple tokenizers Ã— multiple threads = DEADLOCK RISK                  â”‚
â”‚                                                                             â”‚
â”‚ Warning message you'd see:                                                  â”‚
â”‚ "huggingface/tokenizers: The current process just got forked..."           â”‚
â”‚ "Disabling parallelism to avoid deadlocks..."                              â”‚
â”‚                                                                             â”‚
â”‚ The fix:                                                                    â”‚
â”‚ os.environ["TOKENIZERS_PARALLELISM"] = "false"                             â”‚
â”‚ Sets tokenizer to single-threaded mode BEFORE it loads                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

| Modification | When | Effect |
|--------------|------|--------|
| Set to "true" | Single-threaded DataLoader (num_workers=0) | Faster tokenization |
| Keep "false" | Multi-worker DataLoader (num_workers>0) | Prevents deadlocks |

**Recommendation:** Keep it "false" unless you have specific performance issues.

---

## Section 2: Label Configuration

In the current setup, **labels are no longer hardcoded**. Instead, they are passed dynamically via command-line arguments.

### How it works:
- You specify labels using `--label_columns` (e.g., `--label_columns HateSpeech`).
- The framework supports both single-label (binary) and multi-label classification.

### Your Dataset Structure (`HateSpeech.csv`):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXPECTED CSV FORMAT (Single-Label)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Comments,HateSpeech                                                         â”‚
â”‚ "à¦¯à¦¦à¦¿ à¦¦à§‡à¦–à§‹ à¦†à¦•à¦¾à¦¶ à¦­à¦°à¦¾ à¦¤à¦¾à¦°à¦¾...",1                                               â”‚
â”‚ "à¦“à¦° à¦ªà§à¦°à¦¾ à¦—à§à¦·à§à¦Ÿà¦¿ à¦¶à§à¦¬à¦¶à§à¦°à§‡à¦° à¦—à§à¦·à§à¦Ÿà¦¿...",0                                       â”‚
â”‚                                                                             â”‚
â”‚ Important:                                                                  â”‚
â”‚ - Labels are 0 or 1 (binary)                                               â”‚
â”‚ - Column names are flexible (looks for 'Comments', 'comments', 'text', etc.)â”‚
â”‚ - The framework renames the text column to 'comment' internally             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 3: Data Loading and Preprocessing

```python
def load_and_preprocess_data(dataset_path: str, label_columns: List[str]) -> Tuple[np.ndarray, np.ndarray]:
    df = pd.read_csv(dataset_path)
    
    # Column name flexibility
    comment_col = None
    for col in ['Comments', 'comments', 'Comment', 'comment', 'text', 'Text', 'content']:
        if col in df.columns:
            comment_col = col
            break
            
    if comment_col != 'comment':
        df = df.rename(columns={comment_col: 'comment'})
        
    df = df.dropna(subset=['comment'] + label_columns)
    
    comments = df['comment'].values
    labels = df[label_columns].values
    
    return comments, labels
```

### Key Features:
1. **Column Flexibility**: Automatically detects common text column names.
2. **Robustness**: Drops rows with missing text or labels.
3. **Internal Consistency**: Standardizes the text column to `comment` for the rest of the pipeline.

### What This Does:

1. Reads your CSV file
2. Extracts text column
3. Extracts label columns as numpy array
4. Optionally cleans text

### Step-by-Step Walkthrough:

```python
def load_and_preprocess_data(data_path, text_column='comment_text', ...):
    
    # Step 1: Load CSV
    print(f"ğŸ“‚ Loading data from: {data_path}")
    df = pd.read_csv(data_path)
    print(f"   Loaded {len(df)} samples")
    
    # Step 2: Validate columns exist
    if text_column not in df.columns:
        raise ValueError(f"Text column '{text_column}' not found!")
    
    for col in label_columns:
        if col not in df.columns:
            raise ValueError(f"Label column '{col}' not found!")
    
    # Step 3: Extract text
    comments = df[text_column].astype(str).tolist()
    
    # Step 4: Extract labels as numpy array
    labels = df[label_columns].values.astype(np.float32)
    # Shape: (num_samples, num_labels) = (44000, 5)
    
    # Step 5: Optional text cleaning
    if clean_text:
        comments = [clean_bangla_text(text) for text in comments]
    
    # Step 6: Print statistics
    print(f"\nğŸ“Š Label Distribution:")
    for i, col in enumerate(label_columns):
        positive = labels[:, i].sum()
        print(f"   {col}: {positive:.0f} ({positive/len(labels)*100:.1f}%)")
    
    return comments, labels
```

### Visual Data Flow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA LOADING FLOW                                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   CSV File                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ comment_text          â”‚ bully â”‚ sexual â”‚ religious â”‚ ... â”‚           â”‚
â”‚   â”‚ "à¦¤à§à¦®à¦¿ à¦¬à§‹à¦•à¦¾"            â”‚   1   â”‚   0    â”‚     0     â”‚ ... â”‚           â”‚
â”‚   â”‚ "à¦à¦Ÿà¦¾ à¦¸à§à¦ªà§à¦¯à¦¾à¦®"          â”‚   0   â”‚   0    â”‚     0     â”‚ ... â”‚           â”‚
â”‚   â”‚ ...                   â”‚  ...  â”‚  ...   â”‚    ...    â”‚ ... â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                          â†“                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚                    load_and_preprocess_data()                        â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ comments (List)     â”‚    â”‚ labels (numpy array)             â”‚         â”‚
â”‚   â”‚ ["à¦¤à§à¦®à¦¿ à¦¬à§‹à¦•à¦¾",        â”‚    â”‚ [[1, 0, 0, 0, 0],                â”‚         â”‚
â”‚   â”‚  "à¦à¦Ÿà¦¾ à¦¸à§à¦ªà§à¦¯à¦¾à¦®",      â”‚    â”‚  [0, 0, 0, 0, 1],                â”‚         â”‚
â”‚   â”‚  ...]               â”‚    â”‚  ...]                            â”‚         â”‚
â”‚   â”‚ Length: 44000       â”‚    â”‚ Shape: (44000, 5)                â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

| Modification | Code Change | When to Use |
|--------------|-------------|-------------|
| Different text column | `text_column='content'` | Your CSV uses different name |
| Skip cleaning | `clean_text=False` | Text is already preprocessed |
| Add custom cleaning | Modify `clean_bangla_text()` | Need specific preprocessing |

**Example - Custom Text Cleaning:**
```python
def clean_bangla_text(text: str) -> str:
    """Clean Bangla text for processing."""
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    
    # Remove English characters (keep Bangla only)
    # text = re.sub(r'[a-zA-Z]', '', text)  # Uncomment if needed
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    # ADD YOUR CUSTOM CLEANING HERE
    # Example: Remove specific words
    # text = text.replace('spam_word', '')
    
    return text.strip()
```

---

## Section 4: Tokenization Caching (THE MOST IMPORTANT SECTION!)

```python
def get_cache_filename(tokenizer_name: str, max_length: int, cache_dir: str) -> str:
    """Generate unique cache filename based on tokenizer and settings."""
    # Create hash of settings for unique filename
    settings_str = f"{tokenizer_name}_{max_length}"
    settings_hash = hashlib.md5(settings_str.encode()).hexdigest()[:8]
    
    safe_name = tokenizer_name.replace('/', '_')
    filename = f"{safe_name}_maxlen{max_length}_{settings_hash}_tokenized.pkl"
    
    return os.path.join(cache_dir, filename)
```

### What This Does:

Creates a unique filename for cached tokenized data based on:
- Tokenizer name (e.g., "csebuetnlp/banglabert")
- Max sequence length (e.g., 128)

### Why Caching is Critical:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WHY TOKENIZATION CACHING SAVES HOURS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ WITHOUT CACHING:                                                            â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚                                                                             â”‚
â”‚ Experiment 1: python main.py --pipeline baseline                           â”‚
â”‚               â†’ Tokenize 44,000 texts... 90 seconds                        â”‚
â”‚                                                                             â”‚
â”‚ Experiment 2: python main.py --pipeline kd_only                            â”‚
â”‚               â†’ Tokenize 44,000 texts... 90 seconds (AGAIN!)               â”‚
â”‚                                                                             â”‚
â”‚ Experiment 3: python main.py --pipeline kd_prune                           â”‚
â”‚               â†’ Tokenize 44,000 texts... 90 seconds (AGAIN!)               â”‚
â”‚                                                                             â”‚
â”‚ 8 experiments Ã— 90 seconds = 12 MINUTES wasted on tokenization             â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ WITH CACHING:                                                               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚                                                                             â”‚
â”‚ Experiment 1: python main.py --pipeline baseline                           â”‚
â”‚               â†’ Cache miss! Tokenize... 90 seconds                         â”‚
â”‚               â†’ Save to cache/banglabert_maxlen128_tokenized.pkl           â”‚
â”‚                                                                             â”‚
â”‚ Experiment 2: python main.py --pipeline kd_only                            â”‚
â”‚               â†’ Cache hit! Load from cache... 2 seconds                    â”‚
â”‚                                                                             â”‚
â”‚ Experiment 3-8: All cache hits... 2 seconds each                           â”‚
â”‚                                                                             â”‚
â”‚ Total: 90 + (7 Ã— 2) = 104 seconds vs 720 seconds = 7Ã— FASTER!              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Section 4: Tokenization Caching (The Performance Booster)

```python
def get_or_create_tokenized_dataset(
    comments: np.ndarray,
    labels: np.ndarray,
    tokenizer,
    max_length: int,
    cache_dir: str = './cache',
    student_tokenizer = None
) -> Dict[str, torch.Tensor]:
```

### What This Does:
1. **Generates a Unique Key**: Based on the model name and max length.
2. **Dual Tokenization Support**: If a `student_tokenizer` is provided (e.g., when the student uses a different vocabulary than the teacher), it tokenizes for BOTH models and caches them together.
3. **Saves as Torch Tensors**: Uses `torch.save` for efficient storage and loading.

### Why this is a "Game Changer":
Imagine you are running a 5-fold cross-validation with 10 different hyperparameter combinations.
```

### Cache File Structure:

```
cache/
â”œâ”€â”€ csebuetnlp_banglabert_maxlen128_a1b2c3d4_tokenized.pkl  (420 MB)
â”‚   â””â”€â”€ Contains: {
â”‚         'input_ids': tensor of shape (44000, 128),
â”‚         'attention_mask': tensor of shape (44000, 128),
â”‚         'labels': tensor of shape (44000, 5)
â”‚       }
â”‚
â”œâ”€â”€ distilbert-base-multilingual-cased_maxlen128_e5f6g7h8_tokenized.pkl
â”‚   â””â”€â”€ Different tokenizer = different cache file!
â”‚
â””â”€â”€ csebuetnlp_banglabert_maxlen256_i9j0k1l2_tokenized.pkl
    â””â”€â”€ Different max_length = different cache file!
```

### What You Can Modify:

| Modification | Effect | When to Use |
|--------------|--------|-------------|
| Clear cache | Delete files in `cache/` | Dataset changed, force re-tokenization |
| Change cache_dir | `--cache_dir /fast_ssd/cache` | Faster disk for cache files |
| Disable caching | Remove cache logic | Debugging tokenization issues |

**When Cache is Invalidated Automatically:**
- Different tokenizer model
- Different max_length
- Different dataset (detected by filename hash)

**When You MUST Manually Clear Cache:**
- Same filename but different content (rare)
- Tokenizer was updated on HuggingFace

---

## Section 5: PyTorch Dataset Class

```python
class IndexedDataset(Dataset):
    """
    PyTorch Dataset that uses indices to access cached data.
    
    Why indices instead of copying data?
    - Memory efficient: One copy of tokenized data shared across train/val
    - Fast: No data copying when creating train/val splits
    """
    
    def __init__(self, tokenized_data: Dict[str, torch.Tensor], indices: np.ndarray):
        self.tokenized_data = tokenized_data
        self.indices = indices
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        # Get the actual index from our subset
        real_idx = self.indices[idx]
        
        return {
            'input_ids': self.tokenized_data['input_ids'][real_idx],
            'attention_mask': self.tokenized_data['attention_mask'][real_idx],
            'labels': self.tokenized_data['labels'][real_idx]
        }
```

### Why This Design:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MEMORY-EFFICIENT DATA SPLITTING                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ BAD APPROACH (copies data):                                                â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚                                                                             â”‚
â”‚   Full tokenized data: 420 MB                                              â”‚
â”‚         â†“                                                                   â”‚
â”‚   train_data = full_data[train_indices]  â†’ Copy: 340 MB                    â”‚
â”‚   val_data = full_data[val_indices]      â†’ Copy: 80 MB                     â”‚
â”‚                                                                             â”‚
â”‚   Total memory: 420 + 340 + 80 = 840 MB âŒ                                 â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ GOOD APPROACH (uses indices):                                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                             â”‚
â”‚   Full tokenized data: 420 MB (stored once)                                â”‚
â”‚         â†“                                                                   â”‚
â”‚   train_dataset = IndexedDataset(full_data, train_indices)                 â”‚
â”‚                   â””â”€â”€ Just stores: [0, 2, 5, 7, ...]  (~0.3 MB)            â”‚
â”‚                                                                             â”‚
â”‚   val_dataset = IndexedDataset(full_data, val_indices)                     â”‚
â”‚                 â””â”€â”€ Just stores: [1, 3, 4, 6, ...]  (~0.1 MB)              â”‚
â”‚                                                                             â”‚
â”‚   Total memory: 420 + 0.3 + 0.1 = 420.4 MB âœ…                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

**Add data augmentation:**
```python
def __getitem__(self, idx):
    real_idx = self.indices[idx]
    
    input_ids = self.tokenized_data['input_ids'][real_idx].clone()
    
    # AUGMENTATION: Randomly mask some tokens during training
    if self.training and random.random() < 0.1:
        mask_idx = random.randint(1, len(input_ids) - 2)
        input_ids[mask_idx] = self.mask_token_id
    
    return {
        'input_ids': input_ids,
        'attention_mask': self.tokenized_data['attention_mask'][real_idx],
        'labels': self.tokenized_data['labels'][real_idx]
    }
```

**Add sample weighting:**
```python
def __getitem__(self, idx):
    real_idx = self.indices[idx]
    
    labels = self.tokenized_data['labels'][real_idx]
    
    # Higher weight for threat samples (rare but important)
    weight = 5.0 if labels[3] == 1 else 1.0  # labels[3] = threat
    
    return {
        'input_ids': self.tokenized_data['input_ids'][real_idx],
        'attention_mask': self.tokenized_data['attention_mask'][real_idx],
        'labels': labels,
        'weight': weight  # Use in loss calculation
    }
```

---

## Section 6: K-Fold Cross-Validation

```python
def prepare_kfold_splits(
    comments: List[str],
    labels: np.ndarray,
    num_folds: int = 5,
    stratification_type: str = 'multiclass',
    seed: int = 42
) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    Prepare K-fold cross-validation splits with stratification.
    """
```

### What This Does:

Splits your data into K folds for cross-validation, ensuring each fold has similar label distribution.

### Why Stratification Matters:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STRATIFICATION EXPLAINED                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Your dataset label distribution:                                           â”‚
â”‚   bully: 40%                                                               â”‚
â”‚   threat: 5%    â† Rare class!                                              â”‚
â”‚   spam: 30%                                                                â”‚
â”‚   sexual: 15%                                                              â”‚
â”‚   religious: 10%                                                           â”‚
â”‚                                                                             â”‚
â”‚ WITHOUT STRATIFICATION:                                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚                                                                             â”‚
â”‚   Fold 1: threat = 8%   (lucky, got more)                                  â”‚
â”‚   Fold 2: threat = 2%   (unlucky, got fewer)                               â”‚
â”‚   Fold 3: threat = 7%                                                      â”‚
â”‚   Fold 4: threat = 3%                                                      â”‚
â”‚   Fold 5: threat = 5%                                                      â”‚
â”‚                                                                             â”‚
â”‚   Problem: Model trained on Fold 2 never learns threats well!              â”‚
â”‚            Results vary wildly between folds.                              â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ WITH STRATIFICATION:                                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚                                                                             â”‚
â”‚   Fold 1: threat = 5%   (same as original)                                 â”‚
â”‚   Fold 2: threat = 5%   (same as original)                                 â”‚
â”‚   Fold 3: threat = 5%   (same as original)                                 â”‚
â”‚   Fold 4: threat = 5%   (same as original)                                 â”‚
â”‚   Fold 5: threat = 5%   (same as original)                                 â”‚
â”‚                                                                             â”‚
â”‚   Each fold is representative of the whole dataset!                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-label Stratification Challenge:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THE MULTI-LABEL PROBLEM                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ In multi-label classification, a sample can have MULTIPLE labels:          â”‚
â”‚                                                                             â”‚
â”‚   Sample 1: [bully=1, threat=1, spam=0, ...]   â† bully AND threat          â”‚
â”‚   Sample 2: [bully=1, threat=0, spam=0, ...]   â† only bully                â”‚
â”‚   Sample 3: [bully=0, threat=1, spam=0, ...]   â† only threat               â”‚
â”‚                                                                             â”‚
â”‚ Problem: How do you stratify when labels aren't mutually exclusive?        â”‚
â”‚                                                                             â”‚
â”‚ Solutions implemented:                                                      â”‚
â”‚                                                                             â”‚
â”‚ 1. 'multiclass' (default):                                                 â”‚
â”‚    Convert multi-label to single label by finding most common combination  â”‚
â”‚    Good enough for most cases                                              â”‚
â”‚                                                                             â”‚
â”‚ 2. 'multilabel' (advanced):                                                â”‚
â”‚    Use iterative-stratification library                                    â”‚
â”‚    pip install iterative-stratification                                    â”‚
â”‚    Better but slower, requires extra dependency                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation:

```python
## Section 6: K-Fold Cross-Validation

```python
def prepare_kfold_splits(
    comments: np.ndarray,
    labels: np.ndarray,
    num_folds: int = 5,
    stratification_type: str = 'binary',
    seed: int = 42
) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
```

### What This Does:
1. **Stratified Splitting**: For binary classification, it uses `StratifiedKFold` to ensure each fold has the same ratio of HateSpeech vs. Non-HateSpeech as the original dataset.
2. **Reproducibility**: Uses a fixed `seed` so your folds are the same every time you run the experiment.
3. **Memory Efficient**: Uses a `Generator` to yield indices, not the actual data.

### Why use Stratified K-Fold?
If your dataset is imbalanced (e.g., only 10% HateSpeech), a random split might accidentally put all HateSpeech examples in the training set and none in the validation set. Stratification prevents this.
```

### What You Can Modify:

| Modification | Effect | Research Use |
|--------------|--------|--------------|
| `num_folds=10` | More folds, smaller validation sets | More robust estimates |
| `num_folds=3` | Fewer folds, larger validation sets | Faster experiments |
| `stratification_type='multilabel'` | Better stratification | More balanced folds |
| Use single fold | `splits[0]` only | Quick experiments |

**Example - Using all 5 folds for robust evaluation:**
```python
# In main.py, modify to use all folds:
all_fold_metrics = []

for fold_idx, (train_idx, val_idx) in enumerate(splits):
    print(f"\n=== FOLD {fold_idx + 1}/{num_folds} ===")
    
    # Train on this fold
    metrics = train_and_evaluate(train_idx, val_idx)
    all_fold_metrics.append(metrics)

# Report mean Â± std across folds
mean_f1 = np.mean([m.f1_macro for m in all_fold_metrics])
std_f1 = np.std([m.f1_macro for m in all_fold_metrics])
print(f"F1 Macro: {mean_f1:.4f} Â± {std_f1:.4f}")
```

---

## Section 7: Class Weight Calculation

```python
def calculate_class_weights(labels: np.ndarray) -> torch.Tensor:
    labels = labels.ravel()
    pos_counts = np.sum(labels)
    neg_counts = len(labels) - pos_counts
    
    weight = neg_counts / pos_counts if pos_counts > 0 else 1.0
    return torch.FloatTensor([weight])
```

### What This Does:
Calculates a weight for the positive class to handle **class imbalance**. If you have 9,000 non-hate comments and 1,000 hate comments, the weight for "HateSpeech" will be `9.0`.

### How it's used:
This weight is passed to the loss function (`BCEWithLogitsLoss`), telling the model to "pay 9x more attention" when it misses a HateSpeech example.

### Visual Explanation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLASS WEIGHTING FOR IMBALANCED DATA                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Your dataset (44,000 samples):                                             â”‚
â”‚                                                                             â”‚
â”‚ Label      â”‚ Positive â”‚ Negative â”‚ Ratio        â”‚ Weight                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚ bully      â”‚  17,600  â”‚  26,400  â”‚ 40% positive â”‚ 26400/17600 = 1.5        â”‚
â”‚ sexual     â”‚   6,600  â”‚  37,400  â”‚ 15% positive â”‚ 37400/6600  = 5.7        â”‚
â”‚ religious  â”‚   4,400  â”‚  39,600  â”‚ 10% positive â”‚ 39600/4400  = 9.0        â”‚
â”‚ threat     â”‚   2,200  â”‚  41,800  â”‚  5% positive â”‚ 41800/2200  = 19.0 â†!!   â”‚
â”‚ spam       â”‚  13,200  â”‚  30,800  â”‚ 30% positive â”‚ 30800/13200 = 2.3        â”‚
â”‚                                                                             â”‚
â”‚ Effect in loss function:                                                    â”‚
â”‚                                                                             â”‚
â”‚   Without weights:                                                          â”‚
â”‚   - Missing a threat: Loss = 1.0                                           â”‚
â”‚   - Missing a bully:  Loss = 1.0                                           â”‚
â”‚   - Model optimizes for common classes (bully, spam)                       â”‚
â”‚                                                                             â”‚
â”‚   With weights:                                                             â”‚
â”‚   - Missing a threat: Loss = 19.0  (19Ã— more important!)                   â”‚
â”‚   - Missing a bully:  Loss = 1.5                                           â”‚
â”‚   - Model pays attention to rare classes!                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

**Cap maximum weight (prevent instability):**
```python
def calculate_class_weights(labels, max_weight=10.0):
    # ... calculate weights ...
    weights = [min(w, max_weight) for w in weights]  # Cap at 10
    return torch.tensor(weights)
```

**Use different weighting scheme:**
```python
# Option 1: Square root weighting (less aggressive)
weight = np.sqrt(num_negative / num_positive)

# Option 2: Logarithmic weighting (even less aggressive)
weight = np.log1p(num_negative / num_positive)

# Option 3: Effective number weighting (from "Class-Balanced Loss" paper)
beta = 0.9999
effective_num = 1.0 - np.power(beta, num_positive)
weight = (1.0 - beta) / effective_num
```

---

## Section 8: DataLoader Creation

```python
def create_data_loaders(
    tokenized_data: Dict[str, torch.Tensor],
    train_indices: np.ndarray,
    val_indices: np.ndarray,
    batch_size: int = 32,
    num_workers: int = 2
) -> Tuple[DataLoader, DataLoader]:
    """
    Create PyTorch DataLoaders for training and validation.
    """
    
    train_dataset = IndexedDataset(tokenized_data, train_indices)
    val_dataset = IndexedDataset(tokenized_data, val_indices)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,           # Shuffle for training
        num_workers=num_workers,
        pin_memory=True         # Faster GPU transfer
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,          # No shuffle for validation
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader
```

### Understanding DataLoader Parameters:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATALOADER PARAMETERS EXPLAINED                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ batch_size=32:                                                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚   How many samples per forward/backward pass                               â”‚
â”‚                                                                             â”‚
â”‚   Smaller (8-16):  âœ“ Uses less GPU memory                                  â”‚
â”‚                    âœ— Noisier gradients, slower convergence                 â”‚
â”‚                    Use when: GPU memory limited                            â”‚
â”‚                                                                             â”‚
â”‚   Larger (64-128): âœ“ More stable gradients, faster training                â”‚
â”‚                    âœ— Uses more GPU memory                                  â”‚
â”‚                    Use when: Plenty of GPU memory                          â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ shuffle=True (training only):                                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚   Randomizes sample order each epoch                                       â”‚
â”‚                                                                             â”‚
â”‚   Why: Prevents model from memorizing order                                â”‚
â”‚        Forces model to generalize                                          â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ num_workers=2:                                                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚   Number of parallel processes for data loading                            â”‚
â”‚                                                                             â”‚
â”‚   0: Load data in main process (slowest, but safe)                         â”‚
â”‚   2-4: Parallel loading (faster)                                           â”‚
â”‚   >4: Diminishing returns, may cause issues                                â”‚
â”‚                                                                             â”‚
â”‚   Set to 0 if you see deadlocks or memory issues                           â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ pin_memory=True:                                                            â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚   Pre-loads data into GPU-compatible memory                                â”‚
â”‚   Faster CPU â†’ GPU transfer                                                 â”‚
â”‚   Only helps with CUDA, disable for CPU-only                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

| Parameter | Effect | When to Modify |
|-----------|--------|----------------|
| `batch_size` | Memory usage, training speed | GPU out of memory? Reduce it |
| `num_workers` | Data loading speed | Deadlocks? Set to 0 |
| `pin_memory` | GPU transfer speed | CPU only? Set to False |

**Adding gradient accumulation for large effective batch size:**
```python
# In training loop, instead of:
for batch in train_loader:
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# Use accumulation (effective batch = batch_size Ã— accumulation_steps):
accumulation_steps = 4  # Effective batch = 32 Ã— 4 = 128

for i, batch in enumerate(train_loader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## Complete Data Flow Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         COMPLETE DATA FLOW                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Step 1: Load CSV                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ load_and_preprocess_data("HateSpeech.csv", ["HateSpeech"])          â”‚  â”‚
â”‚   â”‚   â†’ comments: np.ndarray (Bengali texts)                            â”‚  â”‚
â”‚   â”‚   â†’ labels: np.ndarray (Binary 0/1)                                 â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â†“                                            â”‚
â”‚   Step 2: Tokenize (with dual caching!)                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ get_or_create_tokenized_dataset(comments, labels, teacher_tok, ...) â”‚  â”‚
â”‚   â”‚   â†’ Check cache: cache/model_maxlen128_tokenized.pkl               â”‚  â”‚
â”‚   â”‚   â†’ If exists: Load in ~2 seconds âœ“                                 â”‚  â”‚
â”‚   â”‚   â†’ If not: Tokenize for Teacher & Student, save to cache           â”‚  â”‚
â”‚   â”‚   â†’ Returns: Dict with input_ids, student_input_ids, etc.           â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â†“                                            â”‚
â”‚   Step 3: Split into folds                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ prepare_kfold_splits(comments, labels, num_folds=5)                â”‚  â”‚
â”‚   â”‚   â†’ Yields: (train_indices, val_indices) for each fold             â”‚  â”‚
â”‚   â”‚   â†’ Stratified: Preserves HateSpeech ratio in each fold            â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â†“                                            â”‚
â”‚   Step 4: Create DataLoaders                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ create_data_loaders(tokenized_data, train_idx, val_idx)            â”‚  â”‚
â”‚   â”‚   â†’ train_loader: Shuffled, for training                           â”‚  â”‚
â”‚   â”‚   â†’ val_loader: Not shuffled, for evaluation                       â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â†“                                            â”‚
â”‚   Step 5: Calculate class weights                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ calculate_class_weights(labels[train_idx])                         â”‚  â”‚
â”‚   â”‚   â†’ weight: [9.0] (example for 10% hate speech)                     â”‚  â”‚
â”‚   â”‚   â†’ Used in BCEWithLogitsLoss(pos_weight=weight)                    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â†“                                            â”‚
â”‚   Ready for training!                                                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary: What You Can Modify in This Script

| Category | Modification | Research Impact |
|----------|--------------|-----------------|
| **Labels** | `--label_columns` argument | Change classification task |
| **Dataset** | `--dataset_path` argument | Use different CSV files |
| **Caching** | `--cache_dir` argument | Faster storage |
| **Folds** | `num_folds`, stratification | Validation robustness |
| **Weighting** | Weight calculation method | Handle imbalance differently |
| **Batching** | `batch_size`, `num_workers` | Memory/speed tradeoff |

---

## Practice Exercise

Before moving to the next script:

1. **Check your CSV**: Open `data/HateSpeech.csv` and verify the column names.
2. **Run with custom labels**: Try running `main.py` with a different label column if you have one:
   ```bash
   python main.py --dataset_path data/HateSpeech.csv --label_columns HateSpeech
   ```
3. **Inspect the cache**: After running, look into the `cache/` directory. Can you identify which file belongs to which model?

---

**Ready for the next script? The next one is `distillation.py` which implements all Knowledge Distillation methods (logit, hidden, attention, multi_level).**

Would you like me to continue?